{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Problem 0: Making sure you're set up with python\n",
    "### You will need to be able to run all of these cells without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 3005.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at the pretty progress bar\n",
      "look at the pretty progress bar\n",
      "look at the pretty progress bar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(3)):\n",
    "    print(\"look at the pretty progress bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135, 145, 155])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure numpy works\n",
    "arr = np.arange(30).reshape(10, 3)\n",
    "arr.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:1235: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(xticklabels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make sure pandas works\n",
    "pd.Series(range(10), index=list(\"abcdefghij\")).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure scikit learn works\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "arr = np.arange(30).reshape(10, 3)\n",
    "new_arr = normalize(arr)\n",
    "(new_arr * new_arr).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "arr = np.array([[1, 2, 1, 0, 5], [3, 4, 1, 0, 5]])\n",
    "assert to_categorical(arr).shape == (2, 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-732fc2c2c5e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "K.clear_session()\n",
    "inpt = Input(shape=(10,))\n",
    "mat = Dense(1, input_shape=(10,))\n",
    "\n",
    "res = mat(K.variable(np.ones((10, 1))))\n",
    "\n",
    "if tf.__version__.startswith(\"1\"):\n",
    "    print(\n",
    "        f\"you have tensorflow version {tf.__version__}.\"\n",
    "        \"Your code may not be compatible with everything we do in class\"\n",
    "    )\n",
    "    result = K.eval(res)\n",
    "else:\n",
    "    result = res.numpy()\n",
    "    assert (result == K.eval(res)).all()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "inpt = Input(shape=(10,))\n",
    "mat = Dense(1, input_shape=(10,))\n",
    "outpt = mat(inpt)\n",
    "\n",
    "model = Model(inpt, outpt)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\")\n",
    "assert model.count_params() == 11\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Problem 1: Company Descriptions from Wikipedia\n",
    "Downloading data from the web is a large part of working with text.\n",
    "Importantly, it can be difficult to find the __right__ data. \n",
    "Here, we will download the company descriptions from wikipedia for the current set of S&P500 companies. \n",
    "\n",
    "## The `wikipedia` package\n",
    "You can install it with `pip install wikipedia`\n",
    "\n",
    "We will use `wikipeda.page` to grab data\n",
    "```python\n",
    "import wikipedia\n",
    "\n",
    "page = wikipedia.page(page_url)\n",
    "print(page.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: get the list of the S&P500 companies\n",
    "### Hint, a `wikipedia` page has a method `html()` which returns the html from the page\n",
    "### pandas has `read_html` https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_url = 'List_of_S%26P_500_companies'\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: For every S&P500 company, get the page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# your code here. We suggest that you save your results\n",
    "# to a json file so that you don't need to re-scrape\n",
    "\n",
    "print(\"found {} results\".format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Content length\n",
    " - For every page, compute the length of the text (how many characters)\n",
    " - What is the mean length(number of characters) for each gics sector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Count how many times `computer` appears in the content for each company\n",
    " - what is the mean number of times it occurs in each sector?\n",
    " - Do these results make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: repeat part 4 with lots of words\n",
    " - try `['computer', 'oil', 'debt', 'drug', 'building', 'food']`\n",
    " - add your own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['computer', 'oil', 'debt', 'drug', 'building', 'food']\n",
    "\n",
    "# one way to do this is to make a dataframe of counts,\n",
    "# initialize all to 0, and then loop through and count the occurences\n",
    "counts = pd.DataFrame(0, index=companies.Symbol.tolist(), columns=words)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.groupby('GICS Sector').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Obtain structured company data using Regex (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_views</th>\n",
       "      <th>intro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>190485</td>\n",
       "      <td>Apple Inc. is an American multinational techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2386</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>40829</td>\n",
       "      <td>American Airlines, Inc. (AA) is a major Americ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id         page_title  page_views                                         intro_text\n",
       "0      856         Apple Inc.      190485  Apple Inc. is an American multinational techno...\n",
       "1     2386  American Airlines       40829  American Airlines, Inc. (AA) is a major Americ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df = pd.read_csv('kdwd_r1k_articles.csv')\n",
    "wiki_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Write a regex to find unusually capitalized terms\n",
    "Sometimes product names will have unusual capitalization such as iPhone or ThinkPad. Find a list of such terms and investigate if you think some of them are products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2182 terms found that are potential products\n"
     ]
    }
   ],
   "source": [
    "maybe_products_ptn = r'[\\w][A-Z]' # find the correct regular expression here\n",
    "\n",
    "\n",
    "# code here!\n",
    "import numpy as np\n",
    "\n",
    "maybe_products_set = np.array([])\n",
    "for _, row in wiki_df.iterrows():\n",
    "    splitedWordsList = re.split(r'[^\\w]', row['intro_text'])\n",
    "    for word in splitedWordsList:\n",
    "        m = re.search(maybe_products_ptn, word)\n",
    "        if m: maybe_products_set = np.append(maybe_products_set, word)\n",
    "\n",
    "print(len(maybe_products_set), 'terms found that are potential products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we found some of the key terms\n",
    "assert('iPhone' in maybe_products_set)\n",
    "assert('ThinkPad' in maybe_products_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Parse company acquisition data from plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking to identify the following types of patterns.<br />\n",
    "`'Citrix acquired Sequoia Software Corp'`<br />\n",
    "`'Moody\\'s was acquired by Dun & Bradstreet in 1962.'`<br />\n",
    "The idea here is to look for patterns around the word 'acquire' with two valid entities on either side, and an option year at the end.<br />\n",
    "<span style=\"color:orange\">Helpful Reminder:</span> you can create non-capturing capture groups via `(?:capture this|or that)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16375 terms found that are potential companies\n"
     ]
    }
   ],
   "source": [
    "# find a way to capture, 'FedEx', 'Coca-Cola', 'Sequoia Software Corp', 'Dun & Bradstreet' and 'Moody\\'s'\n",
    "company_ptn = r'\\b[A-Z][\\w]*\\'?[\\w]*(?:(?:(?:\\s\\&\\s|\\s)|\\-)+[A-Z][\\w]*\\b)*'  # write pattern\n",
    "\n",
    "maybe_companies_set = np.array([])\n",
    "\n",
    "# code here!\n",
    "for _, row in wiki_df.iterrows():\n",
    "    newList = re.findall(company_ptn, row['intro_text'])\n",
    "    maybe_companies_set = np.concatenate((maybe_companies_set, newList), axis=0)\n",
    "\n",
    "print(len(maybe_companies_set), 'terms found that are potential companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we found some of the key terms\n",
    "assert('FedEx' in maybe_companies_set)\n",
    "assert('Coca-Cola' in maybe_companies_set)\n",
    "assert('Sequoia Software Corp' in maybe_companies_set)\n",
    "assert('Dun & Bradstreet' in maybe_companies_set)\n",
    "assert('Moody\\'s' in maybe_companies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquisition pattern\n",
    "acquisition_ptn = r'(?:acquired|was acquired by)'\n",
    "\n",
    "# find a way to optionally capture the year such as ' in 1962'\n",
    "optional_year_ptn = r'(?: in [12][0-9]{3}\\b)?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assemble the patterns together to a full capture pattern\n",
    "full_acquisition_pattern = (\n",
    "    company_ptn + r'\\s+' + acquisition_ptn + r'\\s+' + company_ptn + optional_year_ptn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 potential acquisitions found.\n"
     ]
    }
   ],
   "source": [
    "acquisition_strings = []\n",
    "for _, row in wiki_df.iterrows():\n",
    "    acquisition_strings.extend(re.findall(full_acquisition_pattern, row['intro_text']))\n",
    "print(len(acquisition_strings), 'potential acquisitions found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we found some of the key terms\n",
    "assert('Citrix acquired Sequoia Software Corp' in acquisition_strings)\n",
    "assert('Moody\\'s was acquired by Dun & Bradstreet in 1962' in acquisition_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Question: Are there any failures false positives in your results? If so, how could you account for them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Citrix acquired Sequoia Software Corp',\n",
       " 'Citrix acquired Framehawk',\n",
       " 'Aflac acquired Continental American Insurance Company',\n",
       " 'Giannini acquired Banca',\n",
       " 'It was acquired by Avago Technologies in 2016',\n",
       " \"Moody's was acquired by Dun & Bradstreet in 1962\",\n",
       " 'Brookfield acquired Rouse Properties in 2016',\n",
       " 'Medtronic acquired Irish',\n",
       " 'ACE Limited acquired Chubb in 2016',\n",
       " 'Lottomatica acquired Gtech Corporation',\n",
       " 'CIT Group acquired OneWest Bank',\n",
       " 'Jacobs acquired CH2M Hill',\n",
       " 'Advance Auto Parts acquired Carport Auto Parts',\n",
       " 'Advance acquired Discount Auto Parts',\n",
       " 'Advance Auto Parts acquired BWP Distributors',\n",
       " 'Thermo Fisher acquired Life Technologies Corporation',\n",
       " 'Bell Atlantic acquired GTE',\n",
       " 'It was acquired by FIS',\n",
       " 'Keurig Green Mountain acquired Dr Pepper Snapple Group',\n",
       " 'Gendex Corporation acquired Dentsply International Inc',\n",
       " 'Sovran Self Storage acquired LifeStorage LP',\n",
       " \"Novartis acquired Incyte's\",\n",
       " 'Colfax acquired British Charter International PLC']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# brief answer here\n",
    "acquisition_strings\n",
    "# Two \"It\" Appear in the list, which can be seen as false positives as \"It\" only refers to a company but is not it's name.\n",
    "# Perhaps one way to account for it is to remove common stop words in maybe_companies_set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Let's look into speed of regex matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a simple pattern of your choice to search for in our dataset\n",
    "search_ptn = r'iPhone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble a list of strings\n",
    "doc_list = wiki_df['intro_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41 ms ± 50.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# evaluation loop here\n",
    "for text in doc_list:\n",
    "    re.search(search_ptn, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compile the regex and see if this increases the speed using `re.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907 µs ± 23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "compile_ptn = re.compile(r'iPhone')\n",
    "# evaluation loop here\n",
    "for text in doc_list:\n",
    "    compile_ptn.search(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on speed: basic string operations are always faster than regex\n",
    "#### show this using `'my_string' in 'other_string'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.6 ns ± 2.97 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "'my_string' in 'other_string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 ns ± 39.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "re.search('my_string','other_string') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Key corporate concepts (20%)\n",
    "\n",
    "### In this problem we would like to look at how important certain key terms are across various different companies in the wikipedia dataset\n",
    "\n",
    "### Do this for the terms `apple`, `cloud`, `healthcare` and `personal computers`\n",
    "### Which companies use each of these terms the most?\n",
    "\n",
    "### Hint: you can do this several different ways, but you may want to consider tf-idf, since it will help you for the next Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here!\n",
    "# From lecture 1 & 2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    \n",
    "    analyzer='word',\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    \n",
    "    lowercase=True,\n",
    "    stop_words=None,\n",
    "\n",
    "    ngram_range=(1, 2),\n",
    "    \n",
    "    min_df=3,\n",
    "    max_df=1.0,\n",
    "    \n",
    ")\n",
    "count_matrix = vectorizer.fit_transform(wiki_df['intro_text'].tolist());\n",
    "vocab = list(pd.Series(vectorizer.vocabulary_).sort_values().index)\n",
    "\n",
    "def get_tf_idf_counts(n_row):\n",
    "    # get number of occurences of ngrams that are not zero\n",
    "    data = []\n",
    "    for idx in count_matrix[n_row].nonzero()[1]:\n",
    "        n_count = count_matrix[n_row, idx]\n",
    "        data.append((n_count, vocab[idx]))\n",
    "    ngram_counts = pd.DataFrame(data, columns=['tfidf_score', 'token']).sort_values(by='tfidf_score', ascending=False)\n",
    "    return ngram_counts.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the keywords\n",
    "keyTerms3 = np.array(['apple', 'cloud', 'healthcare', 'personal computers'])\n",
    "\n",
    "# Create a data frame to store the tf-idf scores of the key terms for each company\n",
    "tf_idfScores = pd.DataFrame(np.column_stack([wiki_df['page_title'], np.zeros( shape = (len(wiki_df['page_title']), len(keyTerms3)) )]), \n",
    "                               columns=append([\"Company\"], keyTerms3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>apple</th>\n",
       "      <th>cloud</th>\n",
       "      <th>healthcare</th>\n",
       "      <th>personal computers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>0.714075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0234897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0881501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coca-Cola</td>\n",
       "      <td>0.0281244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Company      apple cloud healthcare personal computers\n",
       "0              Apple Inc.   0.714075     0          0          0.0234897\n",
       "1       American Airlines          0     0          0                  0\n",
       "2  Advanced Micro Devices          0     0          0          0.0881501\n",
       "3                  Anthem          0     0          0                  0\n",
       "4               Coca-Cola  0.0281244     0          0                  0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for n_row in np.arange(wiki_df.shape[0]):\n",
    "    ngram_counts = get_tf_idf_counts(n_row)\n",
    "    for term in keyTerms3:\n",
    "        rank = np.asarray(np.where(ngram_counts.loc[:, 'token'] == term))\n",
    "        if rank.size > 0: \n",
    "            # Update the score\n",
    "            tf_idfScores.loc[n_row, term] = ngram_counts.iloc[rank[0][0],0]\n",
    "        \n",
    "tf_idfScores.head(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Company      apple      cloud healthcare personal computers\n",
      "0            Apple Inc.   0.714075          0          0          0.0234897\n",
      "14                Xerox   0.189213          0          0                  0\n",
      "195              Garmin   0.129485          0          0                  0\n",
      "8             Microsoft   0.100082  0.0542954          0          0.0680391\n",
      "86         Corning Inc.  0.0799841          0          0                  0\n",
      "66      Electronic Arts  0.0605762          0          0          0.0617729\n",
      "79   Berkshire Hathaway  0.0490558          0          0                  0\n",
      "7                 Intel  0.0403504          0          0          0.0411475\n",
      "4             Coca-Cola  0.0281244          0          0                  0\n",
      "505        Kimco Realty          0          0          0                  0 \n",
      "\n",
      "           Company apple     cloud healthcare personal computers\n",
      "460           Okta     0  0.425402          0                  0\n",
      "321    F5 Networks     0  0.272814          0                  0\n",
      "677        Nutanix     0  0.195676          0                  0\n",
      "565    CenturyLink     0  0.162139          0                  0\n",
      "613         Twilio     0  0.140929          0                  0\n",
      "610     ServiceNow     0  0.129925          0                  0\n",
      "496  Workday, Inc.     0  0.116813          0                  0\n",
      "723  Veeva Systems     0  0.112316          0                  0\n",
      "556    RingCentral     0  0.111824          0                  0\n",
      "78          VMware     0  0.100927          0                  0 \n",
      "\n",
      "                                Company apple cloud healthcare personal computers\n",
      "742            Medical Properties Trust     0     0   0.318664                  0\n",
      "734                   Acadia Healthcare     0     0   0.227144                  0\n",
      "733                 Envision Healthcare     0     0   0.226005                  0\n",
      "575                   Molina Healthcare     0     0   0.161708                  0\n",
      "576               Healthpeak Properties     0     0   0.159204                  0\n",
      "282                              Ecolab     0     0   0.136404                  0\n",
      "351                             Perrigo     0     0   0.133675                  0\n",
      "348  The Interpublic Group of Companies     0     0   0.117723                  0\n",
      "707                           Welltower     0     0   0.116266                  0\n",
      "260                  UnitedHealth Group     0     0   0.115982                  0 \n",
      "\n",
      "                      Company      apple      cloud healthcare personal computers\n",
      "728         Dell Technologies          0          0          0           0.109135\n",
      "2      Advanced Micro Devices          0          0          0          0.0881501\n",
      "8                   Microsoft   0.100082  0.0542954          0          0.0680391\n",
      "66            Electronic Arts  0.0605762          0          0          0.0617729\n",
      "708                   HP Inc.          0          0          0          0.0525155\n",
      "457                  Facebook          0          0          0          0.0466999\n",
      "7                       Intel  0.0403504          0          0          0.0411475\n",
      "0                  Apple Inc.   0.714075          0          0          0.0234897\n",
      "610                ServiceNow          0   0.129925          0                  0\n",
      "720  Monolithic Power Systems          0  0.0717942          0                  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "# Show the top 10 for each term\n",
    "for term in keyTerms3:\n",
    "    tf_idfScores.sort_values(by=[term], inplace=True, ascending=False)\n",
    "    print(tf_idfScores.head(10),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Measure company similarity through word overlap (20%)\n",
    "\n",
    "### We would like to understand how similar different companies are based on the words used in their wikipedia description. Presumably a company providing shipping services will have few words overlapping with a technology company. The goal of this exercise is to group companies based on their description and see if we can recover groups that resemble industry sectors.\n",
    "\n",
    "### Question: There are a few ways of doing this. Two ways that we learned, are counting (using CountVectorizer) or using the tfidf score (using TfidfVectorizer). What are the main differences between these methods for this exercise and why are you choosing to use one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the cosine similarity between words from Apple and Coca-Cola as well as Apple and Microsoft. Which pair is more similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_row_nr = 0\n",
    "cocacola_row_nr = 4\n",
    "microsoft_row_nr = 8\n",
    "\n",
    "# get similarity between each of these\n",
    "\n",
    "# code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's look at a fullblown NxN similarity matrix. Luckily scikit-learn has this optimized for sparse matrices, so we don't need to worry about computation explosion. A nice way to look at the resulting similarity matrix between our companies is a method called hierarchical clustering. (find more information here: https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "\n",
    "### The seaborn library has nice built-in support for doing this: https://seaborn.pydata.org/generated/seaborn.clustermap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# code for plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To investigate the output a little further let's zoom into the top left where we can find the most strongly clustered companies. Question: which companies are these and what industry sector do they belong to?\n",
    "\n",
    "### Hint: You can access the clustered data from the seaborn plot using `.data2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
